import time, random
from bs4 import BeautifulSoup
from urllib.parse import urljoin

# C√°c domain ho·∫∑c chu·ªói c·∫ßn b·ªè qua
BLACKLIST = [
    "facebook", "linkedin", "twitter", "youtube",
    "instagram", "mailto:", "tel:", ".pdf", "#"
]

# --- Auto scroll d√πng cho ki·ªÉu infinite scroll ---
def auto_scroll(driver, pause=2):
    """Cu·ªôn xu·ªëng li√™n t·ª•c cho ƒë·∫øn khi kh√¥ng c√≤n n·ªôi dung m·ªõi."""
    last_height = driver.execute_script("return document.body.scrollHeight")
    while True:
        driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
        time.sleep(pause)
        new_height = driver.execute_script("return document.body.scrollHeight")
        if new_height == last_height:
            break
        last_height = new_height


# ---------------- LAYER 1: L·∫•y danh s√°ch link ----------------
def get_links(driver, base_url, keyword_filter, page_type):
    """
    C√†o to√†n b·ªô link ph√π h·ª£p v·ªõi c·∫•u tr√∫c c·ªßa site:
    - listing (ch·ªâ 1 trang)
    - pagination (t·ª± ƒë·ªông l·∫∑p t·ªõi khi h·∫øt th·∫≠t)
    - infinite_scroll (cu·ªôn t·ªõi ƒë√°y)
    """
    print(f"üåç Crawling {base_url}")
    all_links = set()

    driver.get(base_url)
    time.sleep(3)

    # --- H√†m ki·ªÉm tra link h·ª£p l·ªá ---
    def is_valid(href):
        if not href:
            return False
        if any(bad in href for bad in BLACKLIST):
            return False
        return keyword_filter in href

    # --- Infinite Scroll ---
    if page_type == "infinite_scroll":
        auto_scroll(driver)

    # --- Pagination ---
    if page_type == "pagination":
        page = 1
        empty_streak = 0
        max_empty = 3        # D·ª´ng sau 3 trang li√™n ti·∫øp kh√¥ng c√≥ link m·ªõi
        max_pages = 999      # Gi·ªõi h·∫°n c·ª©ng ƒë·ªÉ ch·ªëng v√≤ng l·∫∑p v√¥ h·∫°n

        while True:
            page_url = f"{base_url}?page={page}" if "?" not in base_url else f"{base_url}&page={page}"
            print(f"üîÑ Visiting {page_url}")

            # --- Retry load trang ---
            for attempt in range(3):
                try:
                    driver.get(page_url)
                    time.sleep(random.uniform(2, 4))
                    break
                except Exception as e:
                    print(f"   ‚ö†Ô∏è Retry {attempt+1}/3 due to {e}")
                    time.sleep(2)

            soup = BeautifulSoup(driver.page_source, "html.parser")
            found_links = [
                urljoin(base_url, a["href"])
                for a in soup.select("a[href]")
                if is_valid(a.get("href"))
            ]

            new_links = [u for u in found_links if u not in all_links]
            all_links.update(new_links)

            print(f"   ‚û§ Page {page}: found={len(found_links)}, new={len(new_links)}, total={len(all_links)}")

            # --- D·ª´ng khi 3 trang li√™n ti·∫øp kh√¥ng c√≥ link m·ªõi ---
            if not new_links:
                empty_streak += 1
                print(f"   ‚ö†Ô∏è Empty page streak = {empty_streak}")
            else:
                empty_streak = 0

            if empty_streak >= max_empty:
                print("   ‚úÖ No new links after 3 consecutive pages ‚Üí stop full crawl.")
                break

            # --- Gi·ªõi h·∫°n an to√†n ---
            page += 1
            if page > max_pages:
                print("   ‚õî Max pages (failsafe 999) ‚Üí stop to avoid infinite loop.")
                break

        print(f"‚úÖ Finished crawl: total {len(all_links)} links from {base_url}")

    else:
        # --- Listing / Single Page ---
        soup = BeautifulSoup(driver.page_source, "html.parser")
        for a in soup.select("a[href]"):
            href = a.get("href", "")
            if is_valid(href):
                all_links.add(urljoin(base_url, href))
        print(f"‚úÖ Collected {len(all_links)} links from listing page.")

    return list(all_links)


# ---------------- LAYER 2: L·∫•y chi ti·∫øt ----------------
def scrape_detail(driver, url):
    """L·∫•y n·ªôi dung HTML + meta c·ªßa t·ª´ng link chi ti·∫øt."""
    try:
        driver.get(url)
        time.sleep(random.uniform(2, 4))

        html = driver.page_source
        soup = BeautifulSoup(html, "html.parser")

        title = soup.select_one("h1,h2,h3")
        meta = {m.get("name"): m.get("content") for m in soup.select("meta[name]")}

        return {
            "url": url,
            "title": title.get_text(strip=True) if title else "",
            "meta": meta,
            "html": html,
        }
    except Exception as e:
        print(f"‚ùå Error scraping {url}: {e}")
        return {"url": url, "error": str(e)}
