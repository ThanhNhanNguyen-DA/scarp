from driver_setup import get_selenium_driver
from scraper_logic import get_links, scrape_detail
from data_saver import save_data
import time
from config import SITES_CONFIG

def run_dynamic_scraper():
    driver = get_selenium_driver(headless=False)
    all_results = []

    try:
        for site, cfg in SITES_CONFIG.items():
            base_url = cfg["base_url"]
            keyword_filter = cfg["keyword_filter"]
            page_type = cfg["page_type"]

            print(f"\nüöÄ START SCRAPING: {site}")
            print("=" * 80)

            # 1Ô∏è‚É£ L·∫•y danh s√°ch link
            links = get_links(driver, base_url, keyword_filter, page_type)
            print(f"üîó Total {len(links)} links found for {site}")

            # 2Ô∏è‚É£ C√†o chi ti·∫øt t·ª´ng link
            for i, link in enumerate(links, 1):
                print(f"   [{i}/{len(links)}] Scraping {link}")
                try:
                    data = scrape_detail(driver, link)
                    all_results.append(data)
                except Exception as e:
                    print(f"   ‚ö†Ô∏è Error scraping {link}: {e}")
                    continue

                # Gi·ªõi h·∫°n an to√†n (n·∫øu web qu√° l·ªõn)
                if i >= 3000:
                    print(f"   ‚ö†Ô∏è Safety stop: reached 3000 links for {site}")
                    break

            # 3Ô∏è‚É£ L∆∞u d·ªØ li·ªáu
            save_data(site, all_results)
            print(f"‚úÖ Done {site} ‚Üí {len(all_results)} records saved.")
            all_results.clear()

            # Ngh·ªâ nh·∫π gi·ªØa c√°c site tr√°nh rate-limit
            time.sleep(3)

    except Exception as e:
        print(f"‚ùå Critical error: {e}")
    finally:
        driver.quit()
        print("\nüéØ Finished all sites successfully!")


# ======================
# CH·∫†Y CH∆Ø∆†NG TR√åNH
# ======================
if __name__ == "__main__":
    run_dynamic_scraper()